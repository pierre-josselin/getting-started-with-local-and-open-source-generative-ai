# Guide d'initiation √† l'IA g√©n√©rative locale et open source

Je ne suis pas un expert de l'intelligence artificielle, simplement quelqu'un de passionn√© qui par ce guide va tenter de vous aider √† appr√©hender ce saut dans l'inconnu qu'est l'IA open source.

- [Guide d'initiation √† l'IA g√©n√©rative locale et open source](#guide-dinitiation-√†-lia-g√©n√©rative-locale-et-open-source)
  - [Avant de commencer](#avant-de-commencer)
    - [Qu'est-ce que l'IA ?](#quest-ce-que-lia-)
    - [Pourquoi locale ?](#pourquoi-locale-)
    - [Pourquoi open source ?](#pourquoi-open-source-)
  - [Le mat√©riel](#le-mat√©riel)
    - [La (ou les) carte(s) graphique(s)](#la-ou-les-cartes-graphiques)
      - [Le constructeur du GPU](#le-constructeur-du-gpu)
      - [La capacit√© de la m√©moire](#la-capacit√©-de-la-m√©moire)
      - [La bande passante de la m√©moire](#la-bande-passante-de-la-m√©moire)
      - [La performance](#la-performance)
      - [L'architecture](#larchitecture)
      - [La consommation](#la-consommation)
      - [L'interface](#linterface)
      - [La quantit√©](#la-quantit√©)
      - [Le prix](#le-prix)
      - [Exemples](#exemples)
    - [Le processeur](#le-processeur)
    - [La m√©moire](#la-m√©moire)
    - [La carte m√®re](#la-carte-m√®re)
    - [Le stockage](#le-stockage)
    - [L'alimentation](#lalimentation)
  - [Le logiciel](#le-logiciel)
    - [Le syst√®me d'exploitation](#le-syst√®me-dexploitation)
    - [Le pilote de la carte graphique](#le-pilote-de-la-carte-graphique)
    - [Les outils d'IA open source](#les-outils-dia-open-source)
      - [Pinokio](#pinokio)
    - [Les mod√®les d'IA](#les-mod√®les-dia)
      - [Les LLM open source](#les-llm-open-source)
        - [Les comparateurs de LLM](#les-comparateurs-de-llm)
      - [La quantification](#la-quantification)
      - [Estimer la quantit√© de m√©moire requise pour l'ex√©cution d'un mod√®le](#estimer-la-quantit√©-de-m√©moire-requise-pour-lex√©cution-dun-mod√®le)
      - [Les sites de t√©l√©chargement de mod√®les d'IA open source](#les-sites-de-t√©l√©chargement-de-mod√®les-dia-open-source)
    - [Le suivi des performances](#le-suivi-des-performances)
  - [Exemples](#exemples-1)
    - [D√©ployer un chatbot avec une interface similaire √† ChatGPT](#d√©ployer-un-chatbot-avec-une-interface-similaire-√†-chatgpt)
    - [G√©n√©rer des images](#g√©n√©rer-des-images)
  - [Contribution](#contribution)
  - [Cr√©dits](#cr√©dits)

## Avant de commencer

### Qu'est-ce que l'IA ?

> L'intelligence artificielle (IA) est un ensemble de th√©ories et de techniques visant √† r√©aliser des machines capables de simuler l'intelligence humaine. <cite>[Wikipedia](https://fr.wikipedia.org/wiki/Intelligence_artificielle)</cite>

> L'intelligence artificielle g√©n√©rative ou IA g√©n√©rative (IAg ou GenAI) est un type de syst√®me d'intelligence artificielle (IA) capable de g√©n√©rer du texte, des images, des vid√©os ou d'autres m√©dias en r√©ponse √† des requ√™tes (aussi appel√©es invites, ou en anglais prompts). <cite>[Wikipedia](https://fr.wikipedia.org/wiki/Intelligence_artificielle_g%C3%A9n%C3%A9rative)</cite>

### Pourquoi locale ?

**Vie priv√©e**

Lorsque vous utilisez un service en ligne tel que [ChatGPT](https://openai.com/index/chatgpt/), [Gemini](https://gemini.google.com/) ou encore [Midjourney](https://www.midjourney.com/home), vous partagez des donn√©es plus ou moins personnelles. Ces donn√©es sont g√©n√©ralement stock√©es, revendues, ou r√©utilis√©es pour entra√Æner de nouveaux mod√®les.

**Co√ªt**

La plupart de ces services, bien qu'ils offrent g√©n√©ralement une version gratuite limit√©e, imposent de souscrire √† un abonnement, souvent on√©reux, pour acc√©der aux fonctionnalit√©s compl√®tes.

**Disponibilit√©**

Vous d√©pendez de services tiers. En cas de forte affluence ou de panne, vous ne pouvez pas les utiliser.

**Droits d'utilisation du contenu g√©n√©r√©**

Cela d√©pend des conditions d'utilisation du service, mais la plupart du temps le contenu g√©n√©r√© ne vous appartiens pas, vous ne pouvez pas l'utiliser librement.

D√©ployer ses propres services d'IA localement permet de palier √† tous ces probl√®mes. Bien-s√ªr, c'est un peu plus compliqu√© que d'utiliser un service en ligne, mais rien d'insurmontable.

> [!NOTE]  
> Veuillez noter que nous aborderons ici l'IA g√©n√©rative locale dans le sens "√† domicile". Il est tout √† fait possible d'utiliser des serveurs distants h√©berg√©s sur [AWS](https://aws.amazon.com/fr/) ou [Google Cloud Platform](https://cloud.google.com/) par exemple.

### Pourquoi open source ?

Open source signifie qu'un logiciel est partag√© avec son code source accessible √† tous. Cela offre une transparence totale sur son fonctionnement puisque n'importe qui peut analyser son fonctionnement. La plateforme [GitHub](https://github.com/) est majoritairement utilis√©e pour publier le code source mais ce n'est pas la seule qui existe.

> [!NOTE]  
> La plupart des mod√®les d'IA publi√©s de fa√ßon open source sont fournis sans les jeux de donn√©es qui ont servis √† les entra√Æner, ce qui peut repr√©senter des probl√®mes √©thiques et de transparence majeurs.

> [!NOTE]  
> Open source ne veut pas non plus dire que l'on peut faire ce que l'on veut, il est important de consulter la licence de chacun des produits.

## Le mat√©riel

Pour ex√©cuter des mod√®les d'IA localement, vous aurez besoin d'un ordinateur. Il peut s'agir de votre ordinateur personnel ou d'une machine d√©di√©e √† cet usage. Utilisez le diagramme suivant pour faire votre choix.

![](images/decision-diagram-1-fr.png)

Il est possible de g√©n√©rer du contenu par IA de deux fa√ßon diff√©rentes:

- Avec la carte graphique et sa m√©moire d√©di√©e (GPU/VRAM)
- Avec le processeur et la m√©moire vive (CPU/RAM)

La seconde option est fortement d√©conseill√©e puisqu'elle offre des performance nettement plus basses dans la plupart des cas. En effet, alors que le CPU est con√ßu pour effectuer un nombre de t√¢ches simultan√©es limit√©, le GPU, √† l'inverse, est con√ßu pour en effectuer un grand nombre √† la fois, ce qui est particuli√®rement adapt√© √† la g√©n√©ration de contenu par IA.

### La (ou les) carte(s) graphique(s)

Il s'agit du composant le plus important pour de la g√©n√©ration de contenu par IA. De nombreuses sp√©cifications sont √† prendre en compte et nous allons les aborder ici.

#### Le constructeur du GPU

Il existe plusieurs constructeurs de GPU sur le march√© ([Nvidia](https://www.nvidia.com/fr-fr/), [AMD](https://www.amd.com/fr.html), [Intel](https://www.intel.fr/content/www/fr/fr/homepage.html)...). Nvidia est g√©n√©ralement fortement recommand√© car de nombreux outils utilisent des technologies uniquement disponible sur ces GPU.

#### La capacit√© de la m√©moire

Probablement l'√©l√©ment le plus important, la capacit√© de la m√©moire vid√©o ou VRAM d√©terminera la taille des mod√®les que vous pourrez utiliser. Plus elle est √©lev√©e, plus vous pourrez utiliser de gros mod√®les. Si elle est insuffisante, une partie du mod√®le sera transf√©r√©e vers la m√©moire du CPU (si elle est suffisante) est les performances chuteront drastiquement.

#### La bande passante de la m√©moire

Il existe plusieurs types de m√©moire vid√©o (GDDR5, GDDR6, HBM2...) avec des bandes passantes diff√©rentes. Privil√©gier une bande passante √©lev√©e.

#### La performance

Mesur√©e en [FLOPS](https://fr.wikipedia.org/wiki/FLOPS), la performance d√©pend facteurs tels que la fr√©quence et le nombre de coeurs... Une valeur √©lev√©e offrira une vitesse de g√©n√©ration plus rapide.

> [!NOTE]  
> La performance est mesur√©e selon un type de pr√©cision sp√©cifique (double, single, half...). Ne comparer les performances de plusieurs GPU que pour une m√™me pr√©cision.

#### L'architecture

Privil√©giez toujours des architectures r√©centes. En effet, certains outils utilisent des technologies qui ne sont disponible que sur des GPU relativement r√©cents. Aussi, les architectures r√©centes ont une bien meilleure efficacit√© √©nerg√©tique que les anciennes g√©n√©rations.

Liste des architectures NVIDIA (depuis 2012):

| Nom          | Ann√©e |
| ------------ | ----: |
| Blackwell    |  2024 |
| Hopper       |  2022 |
| Ada Lovelace |  2022 |
| Ampere       |  2020 |
| Turing       |  2018 |
| Volta        |  2017 |
| Pascal       |  2016 |
| Maxwell      |  2014 |
| Kepler       |  2012 |

#### La consommation

Il est important de prendre en compte la consommation de la carte graphique puisque cela aura un impact direct sur le co√ªt en √©lectricit√© de l'utilisation des services, en particulier si vous comptez laisser l'ordinateur sous tension 24 heures sur 24 pour √™tre accessible √† tout moment. Une consommation √©lev√©e produira √©galement plus de chaleur. Enfin, vous devrez disposer d'une alimentation correctement dimensionn√©e.

#### L'interface

Les cartes graphiques se connectent √† la carte m√®re g√©n√©ralement via l'interface PCIe (PCI express). Il existe plusieurs g√©n√©rations de PCIe (1.0, 2.0, 3.0...). Toutes les versions sont retro-compatible. Cependant, si vous utilisez une carte graphique PCIe 5.0 sur un port PCIe 3.0 vous ne b√©n√©ficierez pas de vitesses de transfert optimales.

#### La quantit√©

Il est tout √† fait possible d'utiliser plusieurs cartes graphiques afin de cumuler la m√©moire vid√©o et les performances. Cependant tous les outils ne supportent pas cela. Il est aussi possible de les utiliser en parall√®le avec des instances d'outils diff√©rents.

#### Le prix

C'est g√©n√©ralement l'√©l√©ment d√©terminant dans le choix de la carte graphique. Il s'agit de mat√©riel relativement cher, il est donc important de prendre son temps et comparer les offres. Il peut √©galement √™tre int√©ressant de se tourner vers le march√© de l'occasion.

#### Exemples

Voici un tableau comparant les caract√©ristiques techniques de quelques cartes graphiques potentiellement int√©ressantes pour de l'IA g√©n√©rative. Attention cependant, il ne s'agit absolument pas de recommandations.

| Nom                              | M√©moire | Type de m√©moire |  Performance | Architecture        | Consommation | Interface | Prix au lancement |
| -------------------------------- | ------: | --------------- | -----------: | ------------------- | -----------: | --------- | ----------------: |
| NVIDIA H100 PCIe 80 GB           |   80 GB | HBM2e           | 51.22 TFLOPS | Hopper (2022)       |        350 W | PCIe 5.0  |                 - |
| NVIDIA Tesla P40                 |   24 GB | GDDR5           | 11.76 TFLOPS | Pascal (2016)       |        250 W | PCIe 3.0  |            5699 $ |
| NVIDIA GeForce RTX 5090          |   32 GB | GDDR7           | 104.8 TFLOPS | Blackwell (2024)    |        575 W | PCIe 5.0  |            1999 $ |
| NVIDIA GeForce RTX 4090          |   24 GB | GDDR6X          | 82.58 TFLOPS | Ada Lovelace (2022) |        450 W | PCIe 4.0  |            1599 $ |
| NVIDIA GeForce RTX 4060 Ti 16 GB |   16 GB | GDDR6           | 22.06 TFLOPS | Ada Lovelace (2022) |        165 W | PCIe 4.0  |             499 $ |
| AMD Radeon RX 7900 XTX           |   24 GB | GDDR6           | 61.39 TFLOPS | RDNA 3 (2022)       |        355 W | PCIe 4.0  |             999 $ |

Il existe des outils en ligne pour visualiser et comparer les caract√©ristiques techniques de diff√©rents GPU:

- [TechPowerUp - GPU Specs Database](https://www.techpowerup.com/gpu-specs/)
- [Technical City - Classement des cartes graphiques](https://technical.city/fr/video/rating)

> [!WARNING]  
> Attention aux pi√®ges! Certaines cartes graphiques peuvent sembler vraiment int√©ressantes alors qu'elles ne seront peut-√™tre pas adapt√©es √† votre usage. C'est le cas de la Nvidia Tesla K80 24GB qui peut √™tre trouv√©e pour une centaine d'euros sur internet. Premi√®rement elle fait partie de l'architecture Kepler qui date de 2012 ce qui est vraiment ancien, et il s'agit en r√©alit√© de 2 GPU de 12GB qui ne pourront pas pas √™tre utilis√©s simultan√©ment par la plupart des outils. Enfin, ce type de carte professionnelle est fourni avec une dissipation thermique passive. Il est essentiel pr√©voir une ventilation adapt√©e.

### Le processeur

Tant que la capacit√© de la m√©moire vid√©o est suffisante pour le mod√®le utilis√©, l'impact du processeur sur la g√©n√©ration est minime m√™me si ce n'est √©videmment pas un composant √† n√©gliger.

Cependant, pour b√©n√©ficier de vitesses de transfert optimales, assurez-vous que le CPU supporte une version de PCIe sup√©rieure ou √©gale √† celle de la carte graphique et, si applicable, √† celle du SSD. Dans le cas o√π vous comptez utiliser plusieurs cartes graphiques, assurez-vous que le CPU dispose d'un nombre de voies PCIe maximum suffisant. Cela d√©pend de la fa√ßon dont la carte m√®re les utilises, r√©f√©rez-vous √† la fiche technique du processeur et au manuel d'utilisation de la carte m√®re.

### La m√©moire

Tant que la capacit√© de la m√©moire vid√©o est suffisante pour le mod√®le utilis√©, l'impact du la m√©moire (RAM) sur la g√©n√©ration est minime m√™me si ce n'est √©videmment pas un composant √† n√©gliger.

### La carte m√®re

Il est important de choisir une carte m√®re de bonne qualit√©. Si vous souhaitez utiliser plusieurs cartes graphiques, assurez-vous que la carte m√®re dispose de suffisamment de ports PCIe, et pour des vitesses de transfert optimales, qu'elle supporte les versions PCIe des cartes graphiques.

> [!WARNING]  
> Une carte m√®re avec par exemple 4 ports PCIe ne veut pas n√©cessairement dire que 4 cartes graphiques pourront √™tre utilis√©es simultan√©ment, ou utilis√©es simultan√©ment de fa√ßon optimale. R√©f√©rez-vous au manuel d'utilisation de la carte m√®re.

### Le stockage

Le choix du stockage est important puisqu'il d√©finit √† quelle vitesse le mod√®le pourra √™tre charg√© dans la m√©moire. De plus, les mod√®les d'IA font souvent plusieurs giga-octets voire dizaines de giga-octets donc il est important de choisir une capacit√© suffisante. Pr√©f√©rez l'utilisation d'un SSD M.2 PCIe plut√¥t qu'un HDD ou m√™me un SSD SATA.

### L'alimentation

L'alimentation est un composant essentiel qui doit √™tre correctement dimensionn√©, d'autant plus si vous utilisez plusieurs cartes graphiques puissantes. Utilisez un calculateur en ligne pour d√©terminer quelle alimentation est adapt√©e. Assurez-vous √©galement que l'alimentation dispose de suffisamment de c√¢bles d'alimentation PCIe.

Quelques outils de calcul d'alimentation en ligne:

- [MSI](https://www.msi.com/power-supply-calculator)
- [be quiet!](https://www.bequiet.com/en/psucalculator)
- [Cooler Master](https://www.coolermaster.com/en-eu/power-supply-calculator/)
- [PC builds](https://pc-builds.com/power-supply-calculator/)

## Le logiciel

### Le syst√®me d'exploitation

Une fois votre ordinateur assembl√©, la question du choix du syst√®me d'exploitation se pose. Deux principales plateformes s'offrent √† vous: Windows ou Linux. La quasi totalit√© des outils sont compatibles avec les deux. Windows peut √™tre plus simple √† utiliser, les pilotes graphiques y seront plus simples √† installer pour un d√©butant, alors que le choix d'utiliser Linux peut √™tre totalement justifi√© dans une d√©marche "100% libre et open source". De plus, Windows est payant alors que les distributions Linux sont gratuites, encore un √©l√©ment √† prendre en compte. Une fois le choix de la plateforme fait, il faut encore choisir le syst√®me d'exploitation le plus adapt√© √† ses besoins. En voici une liste non exhaustive:

- Windows
  - [Windows 11](https://www.microsoft.com/fr-fr/software-download/windows11)
  - [Windows 10](https://www.microsoft.com/fr-fr/software-download/windows10)
  - [Windows Server 2025](https://www.microsoft.com/fr-fr/evalcenter/evaluate-windows-server-2025)
  - [Windows Server 2022](https://www.microsoft.com/fr-fr/evalcenter/evaluate-windows-server-2022)

- Linux
  - [Debian](https://www.debian.org/index.fr.html)
  - [Ubuntu](https://www.ubuntu-fr.org/)
  - [Fedora](https://fedoraproject.org/fr/)
  - [CentOS](https://www.centos.org/)

Pour la plupart de ces syst√®mes d'exploitation, vous aurez le choix entre une interface graphique ou en ligne de commande. A moins de vraiment savoir ce que vous faites, choisissez l'interface graphique.

### Le pilote de la carte graphique

Une fois le syst√®me d'exploitation install√© et mis √† jour, la premi√®re √©tape sera d'installer le pilote de la (ou les) carte(s) graphique(s). Vous trouverez ci-dessous des liens vers les pages de t√©l√©chargement des pilotes graphiques des trois principaux constructeurs:

- [Nvidia](https://www.nvidia.com/fr-fr/drivers/)
- [AMD](https://www.amd.com/fr/support/download/drivers.html)
- [Intel](https://www.intel.fr/content/www/fr/fr/products/docs/discrete-gpus/arc/software/drivers.html)

### Les outils d'IA open source

Les outils utilisent des mod√®les d'IA pour g√©n√©rer du contenu. En voici une liste non exhaustive:

- [Ollama](https://ollama.com/)
- [Open WebUI](https://openwebui.com/)
- [Stable Diffusion WebUI ](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- [ComfyUI](https://www.comfy.org/)

#### [Pinokio](https://pinokio.computer/)

Pinokio est un outil particulier et tr√®s int√©ressant puisqu'il s'agit d'une sorte de magasin d'outils IA. De nombreux outils IA peuvent √™tre t√©l√©charg√©s, install√©s et lanc√©s en un click depuis Pinokio.

### Les mod√®les d'IA

La taille des mod√®les d'IA se mesure en nombre de param√®tres. Un mod√®le 70b fait 70 milliards de param√®tres, b correspondant √† billion.

#### Les LLM open source

Les LLM (large language models ou grand mod√®les linguistiques) sont des mod√®les d'IA capables de comprendre et de g√©n√©rer du texte. C'est le type de mod√®le utilis√© par ChatGPT ou Gemini, par exemple.

Voici une liste non exhaustive de LLM open source populaires regroup√©s par organisation:

- DeepSeek üá®üá≥
  - deepseek-r1 (1.5b, 7b, 8b, 14b, 32b 70b, 671b)
  - deepseek-v3 (671b)

- Meta üá∫üá∏
  - llama3.3 (70b)
  - llama3.2-vision (11b, 90b)
  - llama3.2 (1b, 3b)
  - codellama (7b, 13b, 34b, 70b)

- Alibaba üá®üá≥
  - qwen2.5 (0.5b, 1.5b, 3b, 7b, 14b, 32b, 72b)
  - qwen2.5-coder (0.5b, 1.5b, 3b, 7b, 14b, 32b)
  - qwq (32b)

- Microsoft üá∫üá∏
  - phi4 (14b)

- Google üá∫üá∏
  - gemma2 (2b, 9b, 27b)
  - codegemma (2b, 7b)

- Mistral AI üá´üá∑
  - mistral (7b)
  - mistral-large (123b)
  - mistral-small (22b)

Comme vous pouvez le voir, certains mod√®les sont sp√©cialis√©s, par exemple dans la g√©n√©ration de code ou la reconnaissance d'image, alors que d'autres se veulent plus g√©n√©ralistes.

Certains mod√®les sont capables de raisonner, c'est √† dire qu'il vont prendre un certains temps pour r√©fl√©chir avant de g√©n√©rer une r√©ponse. C'est le cas des mod√®les deepseek-r1 et qwq.

##### Les comparateurs de LLM

Comme vous avez pu le voir il existe de nombreux mod√®les d'IA open source dans de nombreuses versions diff√©rentes. Comment choisir ? Il existe des sites web permettant de comparer les performances de ces diff√©rents mod√®les. Comparer non seulement les performances globales mais √©galement les performances selon des crit√®res sp√©cifiques (codage, raisonnement, math√©matiques...). Cela permet √©galement de les situer par rapport √† d'autres mod√®les comme ChatGPT par exemple. En voici une liste non exhaustive:

- [LiveBench](https://livebench.ai/)
- [scale](https://scale.com/leaderboard)

#### La quantification

Pour √™tre efficaces, les mod√®les d'IA sont entra√Æn√©s avec des jeux de donn√©es √©normes. Les fichiers en sortie ont donc g√©n√©ralement une taille de plusieurs giga-octets voire dizaines de giga-octets et demandent donc beaucoup de m√©moire et de puissance pour les ex√©cuter. Il est possible de r√©duire drastiquement la taille de ces mod√®les √† l'aide de la quantification. Cela implique une perte de pr√©cision du mod√®le, plus ou moins grande, selon la m√©thode utilis√©e.

Voici par exemple les diff√©rentes m√©thodes de quantifications propos√©e par [llama.cpp](https://github.com/ggerganov/llama.cpp), par taille d√©croissante, pour un mod√®le 7b:

| Nom    | Taille | PPL     | Qualit√©                  | Remarques                        | Recommand√© |
| ------ | ------ | ------- | ------------------------ | -------------------------------- | ---------- |
| F32    | 26.00G | -       | Sans perte               |                                  | Non        |
| F16    | 13.00G | -       | Pratiquement sans perte  |                                  | Non        |
| Q8_0   | 6.70G  | +0.0004 | Perte extr√™mement faible |                                  | Non        |
| Q6_K   | 5.15G  | +0.0044 | perte extr√™mement faible |                                  |            |
| Q5_K   | -      | -       | -                        | Alias pour Q5_K_M                |            |
| Q5_K_M | 4.45G  | +0.0142 | tr√®s faible perte        |                                  | Oui        |
| Q5_K_S | 4.33G  | +0.0353 | faible perte             |                                  | Oui        |
| Q5_1   | 4.70G  | +0.0415 | Faible perte             | Legacy, pr√©f√©rer utiliser Q5_K_M |            |
| Q5_0   | 4.30G  | +0.0796 | √âquilibr√©e               | Legacy, pr√©f√©rer utiliser Q4_K_M |            |
| Q4_K   | -      | -       | -                        | Alias pour Q4_K_M                |            |
| Q4_K_M | 3.80G  | +0.0535 | √âquilibr√©e               |                                  | Oui        |
| Q4_K_S | 3.56G  | +0.1149 | Perte significative      |                                  |            |
| Q4_1   | 3.90G  | +0.1846 | Perte substantielle      | Legacy, pr√©f√©rer utiliser Q3_K_L |            |
| Q4_0   | 3.50G  | +0.2499 | perte tr√®s √©lev√©e        | Legacy, pr√©f√©rer utiliser Q3_K_M |            |
| Q3_K_L | 3.35G  | +0.1803 | Perte substantielle      |                                  |            |
| Q3_K   | -      | -       | -                        | Alias pour Q3_K_M                |            |
| Q3_K_M | 3.06G  | +0.2437 | perte tr√®s √©lev√©e        |                                  |            |
| Q3_K_S | 2.75G  | +0.5505 | perte tr√®s √©lev√©e        |                                  |            |
| Q2_K   | 2.67G  | +0.8698 | Perte extr√™me            |                                  | Non        |

https://github.com/ggerganov/llama.cpp/discussions/2094#discussioncomment-6351796

> [!NOTE]  
> Sur Ollama, depuis la page d'un mod√®le, cliquer sur "Tags" pour voir la liste des versions et quantifications disponibles.

#### Estimer la quantit√© de m√©moire requise pour l'ex√©cution d'un mod√®le

Pour estimer la quantit√© de m√©moire requise pour executer un mod√®le d'IA, la formule suivante peut √™tre utilis√©e:

```
M = (P x (Q/8)) x 1.2
```

- M: La m√©moire vid√©o requise exprim√©e en giga-octets
- P: Le nombre de param√®tres du mod√®les (70 pour un mod√®le 70b)
- Q: Le nombre de bits utilis√©s pour charger le mod√®le (4 pour une quantification Q4_K_M)
- 1.2: Repr√©sente une marge de 20% pour les t√¢ches suppl√©mentaires

Exemple avec le mod√®le llama3.3 en version 70b-instruct-q4_K_M:

```
(70 x (4/8)) * 1.2
```

La quantit√© de m√©moire requise pour ce mod√®le est d'environ *42GB*.

https://modal.com/blog/how-much-vram-need-inference

#### Les sites de t√©l√©chargement de mod√®les d'IA open source

Il existe plusieurs sites web qui mettent √† disposition des mod√®les d'IA open source, en voici une liste non exhaustive:

- [Ollama](https://ollama.com/search)
- [Civitai](https://civitai.com/models)
- [Hugging Face](https://huggingface.co/models)

> [!NOTE]  
> Le site web Ollama ne permet le t√©l√©chargement de mod√®le directement, il faut pour cela passer par l'outil Ollama.

### Le suivi des performances

Lors de la g√©n√©ration de contenu, il est important de garder un oeil sur la consommation des ressources afin de v√©rifier les temp√©ratures ainsi que l'utilisation des diff√©rentes ressources (CPU, RAM, GPU, VRAM...). Sous Windows, il y a pour cela le gestionnaire des t√¢ches, onglet "Performance". Plus sp√©cifiquement pour Nvidia, la commande `nvidia-smi` est disponible sur toutes les plateformes et permet de visualiser les informations sur l'utilisation des cartes graphiques. Enfin, je vous recommande l'outil [nvitop](https://github.com/XuehaiPan/nvitop) qui est tr√®s aboutit et regroupe toutes les informations importantes (pour Nvidia).

![nvitop](images/nvitop.png)
*nvitop*

## Exemples

### D√©ployer un chatbot avec une interface similaire √† ChatGPT

![Open WebUI](images/open-webui.png)
*Open WebUI*

1. T√©l√©charger, installer et ex√©cuter [Ollama](https://ollama.com/download)
2. T√©l√©charger, installer et ouvrir [Pinokio](https://pinokio.computer/)
3. Acc√©der √† la page "Discover" de Pinokio
4. Dans la barre de recherche, saisir "Open WebUI" 
5. Installer Open WebUI
6. Une fois l'installation termin√©e, appuyer sur "Start" dans le volet gauche
7. Une fois le lancement termin√©, appuyer sur "Open Web UI" dans le volet gauche
8. Cr√©er un compte administrateur
9. Dans le menu de s√©lection de mod√®le en haut √† gauche, saisir le nom d'un mod√®le √† t√©l√©charger (par exemple `llama3.2:3b`) 
10. Cliquer sur "R√©cup√©rer "llama3.2:3b" depuis Ollama"
11. Une fois le t√©l√©chargement du mod√®le termin√©, le s√©lectionner dans la liste
12. Saisir un prompt et appuyer sur la touche "Entr√©e"

Vous pouvez facilement g√©rer les mod√®les install√©s √† partir d'une console:

- `ollama run` pour discuter avec un mod√®le directement depuis la console
- `ollama pull` pour t√©l√©charger un mod√®le sans l'√©x√©cuter
- `ollama list` pour lister les mod√®les t√©l√©charg√©s
- `ollama rm` pour supprimer un mod√®le
- `ollama help` pour obtenir la liste compl√®te des commandes

### G√©n√©rer des images

![Stable diffusion WebUI](images/stable-diffusion-webui.png)
*Stable diffusion WebUI*

1. T√©l√©charger, installer et ouvrir [Pinokio](https://pinokio.computer/)
2. Acc√©der √† la page "Discover" de Pinokio
3. Dans la barre de recherche, saisir "Stable Diffusion web UI" 
4. Installer Stable Diffusion web UI
5. Une fois l'installation termin√©e, appuyer sur "Start" dans le volet gauche
6. Une fois le lancement termin√©, appuyer sur "Open Web UI" dans le volet gauche
7. Choisir et t√©l√©charger un mod√®le (checkpoint) `.safetensors` sur le site [Civitai](https://civitai.com/models)
8. Acc√©der au r√©pertoire de Pinokio via l'explorateur de fichiers (g√©n√©ralement `C:\pinokio` sous Windows)
9. D√©placer le mod√®le vers api ‚Üí automatic1111.git ‚Üí app ‚Üí models ‚Üí Stable-diffusion
10. Cliquer sur le bouton pour actualiser la liste des mod√®les
11. S√©lectionner le mod√®le t√©l√©charg√© dans la liste
12. Saisir un prompt et appuyer sur le bouton "Generate"

## Contribution

Toute contribution est bienvenue. Comme pr√©cis√© au d√©but de ce guide, je ne suis pas un expert, donc il peut comporter des erreurs. N'h√©sitez pas √† apporter des rectifications, am√©liorations ou traductions via pull request.

## Cr√©dits

- `README.fr.md`

    R√©dig√© √† la main par Pierre Josselin
